{"cells":[{"cell_type":"markdown","metadata":{"id":"Ue5hxxkdAQJg"},"source":["<img src=\"https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/raw/main/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Procesamiento de lenguaje natural\n","## Word2vect\n","\n","## Alumno: Fux, Santiago Javier (CEIA-6ta Cohorte)\n","### Fecha: 2023-03-08\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"kCED1hh-Ioyf"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"PUbfVnzIIoMj"},"outputs":[],"source":["def cosine_similarity(a, b):\n","    return np.dot(a, b) / (np.linalg.norm(a) * (np.linalg.norm(b)))"]},{"cell_type":"markdown","metadata":{"id":"DMOa4JPSCJ29"},"source":["### Datos"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"RIO7b8GjAC17"},"outputs":[],"source":["corpus = np.array(['que dia es hoy', 'martes el dia de hoy es martes', 'martes muchas gracias'])"]},{"cell_type":"markdown","metadata":{"id":"8WqdaTmO8P1r"},"source":["Documento 1 --> que dia es hoy \\\n","Documento 2 --> martes el dia de hoy es martes \\\n","Documento 3 --> martes muchas gracias"]},{"cell_type":"markdown","metadata":{"id":"FVHxBRNzCMOS"},"source":["### 1 - Obtener el vocabulario del corpus (los términos utilizados)\n","- Cada documento transformarlo en una lista de términos\n","- Armar un vector de términos no repetidos de todos los documentos"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"3ZqTOZzDI7uv"},"outputs":[],"source":["def get_terms(docs):\n","  # init list\n","  term_list = []\n","\n","  for doc in docs:\n","    # get tokens separated by '' and filter to get a single item of each one\n","    terms = doc.split(' ')  \n","    terms = np.unique(terms)\n","\n","    for term in terms:\n","      # if not in the list then add it\n","      if term not in term_list:\n","        term_list.append(term)\n","  return term_list\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RUhH983FI7It"},"source":["### 2- OneHot encoding\n","Data una lista de textos, devolver una matriz con la representación oneHotEncoding de estos"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Os0AAQo6I6Z1"},"outputs":[],"source":["def get_onehot_enc(docs):\n","  # get terms list\n","  term_list = get_terms(docs)\n","  # init final list\n","  res = []\n","  for doc in docs:\n","    # create new array to append\n","    aux = [0 for i in range(len(term_list))]\n","    terms = doc.split(' ')\n","\n","    for term in terms:\n","      try:\n","        aux[term_list.index(term)] = 1\n","      except:\n","        #not found\n","        pass\n","    res.append(aux)\n","  return res"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["[[1, 1, 1, 1, 0, 0, 0, 0, 0],\n"," [1, 1, 1, 0, 1, 1, 1, 0, 0],\n"," [0, 0, 0, 0, 0, 0, 1, 1, 1]]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# test\n","get_onehot_enc(corpus)\n"]},{"cell_type":"markdown","metadata":{"id":"IIyWGmCpJVQL"},"source":["### 3- Vectores de frecuencia\n","Data una lista de textos, devolver una matriz con la representación de frecuencia de estos"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"yqij_7eHJbUi"},"outputs":[],"source":["def get_freq_enc(docs):\n","  # get terms list\n","  term_list = get_terms(docs)\n","  # init final list\n","  res = []\n","  for doc in docs:\n","    # create new array to append\n","    aux = [0 for i in range(len(term_list))]\n","    terms = doc.split(' ')\n","\n","    for term in terms:\n","      try:\n","        aux[term_list.index(term)] += 1\n","      except:\n","        #not found\n","        pass\n","    res.append(aux)\n","  return res"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["[[1, 1, 1, 1, 0, 0, 0, 0, 0],\n"," [1, 1, 1, 0, 1, 1, 2, 0, 0],\n"," [0, 0, 0, 0, 0, 0, 1, 1, 1]]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# test\n","get_freq_enc(corpus)"]},{"cell_type":"markdown","metadata":{"id":"z_Ot8HvWJcBu"},"source":["### 4- TF-IDF\n","Dada una lista de textos, devolver una matriz con la representacion TFIDF"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"waG_oWtpJjRw"},"outputs":[],"source":["def get_idf(docs, term_list):\n","  res = [0 for i in range(len(term_list))]\n","  one_hot_res = get_onehot_enc(docs)\n","  one_hot_sum = np.sum(one_hot_res, axis=0)\n","  res = [np.log10(len(docs) / v) for v in one_hot_sum if v > 0]\n","  return res\n","\n","def get_tfidf_enc(docs):\n","  tf_vals = get_freq_enc(docs)\n","  idf_vals = get_idf(docs, get_terms(docs))\n","  # print(f'tf_vals = {tf_vals}')\n","  # print(f'idf_vals = {idf_vals}')\n","  return np.array(tf_vals) * idf_vals"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["array([[0.17609126, 0.17609126, 0.17609126, 0.47712125, 0.        ,\n","        0.        , 0.        , 0.        , 0.        ],\n","       [0.17609126, 0.17609126, 0.17609126, 0.        , 0.47712125,\n","        0.47712125, 0.35218252, 0.        , 0.        ],\n","       [0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.17609126, 0.47712125, 0.47712125]])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# test\n","res = get_tfidf_enc(corpus)\n","res"]},{"cell_type":"markdown","metadata":{"id":"xMcsfndWJjm_"},"source":["### 5 - Comparación de documentos\n","Realizar una funcion que reciba el corpus y el índice de un documento y devuelva los documentos ordenados por la similitud coseno"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"CZdiop6IJpZN"},"outputs":[],"source":["def get_cosines_similarity(docs, enc_list, idx):\n","  sim_cos = []\n","  for i, doc in enumerate(docs):\n","    aux = cosine_similarity(enc_list[idx], enc_list[i])\n","    sim_cos.append(aux)\n","  sim_cos.sort(reverse=True)\n","  return sim_cos\n","\n","def compare_docs(docs, idx):\n","  #init list to save cos\n","  sim_cos = []\n","  # get list of encodings\n","  onehot_enc_list = get_onehot_enc(docs)\n","  freq_enc_list = get_freq_enc(docs)\n","  tfidf_enc_list = get_tfidf_enc(docs)\n","  # calculate similarity\n","  onehot_cos_list = get_cosines_similarity(docs, onehot_enc_list, idx)\n","  freq_cos_list = get_cosines_similarity(docs, freq_enc_list, idx)\n","  tfidf_cos_list = get_cosines_similarity(docs, tfidf_enc_list, idx)\n","  print(f'---DOC {idx}---')\n","  print(f'-->One Hot Encoding: {onehot_cos_list}')\n","  print(f'-->Freq Encoding: {freq_cos_list}')\n","  print(f'-->TFIDF Encoding: {tfidf_cos_list}\\n')"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["---DOC 0---\n","-->One Hot Encoding: [1.0, 0.6123724356957946, 0.0]\n","-->Freq Encoding: [1.0, 0.5, 0.0]\n","-->TFIDF Encoding: [0.9999999999999998, 0.20034190268098703, 0.0]\n","\n","---DOC 1---\n","-->One Hot Encoding: [1.0000000000000002, 0.6123724356957946, 0.23570226039551587]\n","-->Freq Encoding: [1.0, 0.5, 0.3849001794597505]\n","-->TFIDF Encoding: [1.0, 0.20034190268098703, 0.10845711727883083]\n","\n","---DOC 2---\n","-->One Hot Encoding: [1.0000000000000002, 0.23570226039551587, 0.0]\n","-->Freq Encoding: [1.0000000000000002, 0.3849001794597505, 0.0]\n","-->TFIDF Encoding: [0.9999999999999999, 0.10845711727883083, 0.0]\n","\n"]}],"source":["\n","compare_docs(corpus, 0)\n","compare_docs(corpus, 1)\n","compare_docs(corpus, 2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO5fRYTpympAwJSVbric6dW","collapsed_sections":[],"name":"1a - word2vec.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
